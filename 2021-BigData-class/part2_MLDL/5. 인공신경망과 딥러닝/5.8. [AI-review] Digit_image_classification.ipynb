{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Digit image classification review",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Scb4NQe2zCPP"
      },
      "source": [
        "# **Digit 데이터를 이용한 <span style=\"color:darkgreen\">뉴럴네트워크</span> 문제**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFKNyCnjSNL6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ef6a0aa-7ee6-40bb-9f06-3731f6e746e7"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Sep  3 06:11:57 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   69C    P8    33W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyVZUOyPZlkN"
      },
      "source": [
        "> **<span style=\"color:red\">다음 문항을 풀기 전에 </span>아래 코드를 실행하시오.**<br>\n",
        "> 반드시 코드와 주석을 읽고 문제를 푸시오. <br>\n",
        "> 출력된 데이터 설명을 **반드시** 읽고 문제를 푸시오.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiSDKfH9abY7",
        "outputId": "8ca17887-f47f-4ef3-daac-c5b6d8037666"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "digits = load_digits()\n",
        "\n",
        "x = digits.images # 인풋으로 사용할 데이터.\n",
        "y = digits.target # 아웃풋으로 사용할 데이터.\n",
        "\n",
        "print(digits.DESCR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. _digits_dataset:\n",
            "\n",
            "Optical recognition of handwritten digits dataset\n",
            "--------------------------------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 5620\n",
            "    :Number of Attributes: 64\n",
            "    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n",
            "    :Missing Attribute Values: None\n",
            "    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n",
            "    :Date: July; 1998\n",
            "\n",
            "This is a copy of the test set of the UCI ML hand-written digits datasets\n",
            "https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
            "\n",
            "The data set contains images of hand-written digits: 10 classes where\n",
            "each class refers to a digit.\n",
            "\n",
            "Preprocessing programs made available by NIST were used to extract\n",
            "normalized bitmaps of handwritten digits from a preprinted form. From a\n",
            "total of 43 people, 30 contributed to the training set and different 13\n",
            "to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n",
            "4x4 and the number of on pixels are counted in each block. This generates\n",
            "an input matrix of 8x8 where each element is an integer in the range\n",
            "0..16. This reduces dimensionality and gives invariance to small\n",
            "distortions.\n",
            "\n",
            "For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\n",
            "T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\n",
            "L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n",
            "1994.\n",
            "\n",
            ".. topic:: References\n",
            "\n",
            "  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n",
            "    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n",
            "    Graduate Studies in Science and Engineering, Bogazici University.\n",
            "  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n",
            "  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n",
            "    Linear dimensionalityreduction using relevance weighted LDA. School of\n",
            "    Electrical and Electronic Engineering Nanyang Technological University.\n",
            "    2005.\n",
            "  - Claudio Gentile. A New Approximate Maximal Margin Classification\n",
            "    Algorithm. NIPS. 2000.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfPaVyBmSuhJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12f2884f-9fc5-400e-eaa8-de2ccb84697f"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1797, 8, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNC1pCeBSukF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cada8078-0a77-489c-c4f0-18b8108c6faa"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1797,)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYPLRcJsS-MM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "c9959e6b-8dc8-471c-db89-27c271dff44d"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.imshow(x[101], cmap='gray')\n",
        "plt.show()\n",
        "print(\"label :\", y[101])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label : 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heKPp9gV0che"
      },
      "source": [
        "# Q1. 다음 조건에 맞추어 데이터를 분할하시오.\n",
        "---------------------------\n",
        "* 변수명 규칙 : x_train, x_test, y_train, y_test\n",
        "* train : test = 9 : 1\n",
        "* y의 클래스가 골고루 분할이 되도록 stratify하게 분할한다.\n",
        "* random state, seed등은 2021로 고정.\n",
        "---------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaW-TZha1QFX"
      },
      "source": [
        "####################\n",
        "## your code here ##\n",
        "####################\n",
        "\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "\n",
        "x_train, x_test, y_train, y_test = tts(x, y, test_size = 0.1, stratify=y, random_state=2021)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkYpw0PoTraA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66c8d1f6-f116-43b8-eada-ec05901374b5"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1617, 8, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdzcPhafTtKx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ece871b7-69c9-4563-994b-0c727d9e3a46"
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(180, 8, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oviV2HIh1ePP"
      },
      "source": [
        "# Q2. 모든 x들을 min-max scaling 하시오.\n",
        "---------------------------\n",
        "* 모든 트레이닝 규칙은 트레이닝 셋을 이용하여 찾아낸다.\n",
        "* 제대로 스케일링 되었는지, 결과도 확인한다.\n",
        "---------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk5q8Ema1ePR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee15d5b5-6564-4183-9e87-634e15e44589"
      },
      "source": [
        "####################\n",
        "## your code here ##\n",
        "####################\n",
        "\n",
        "max_ = x_train.max()\n",
        "min_ = x_train.min()\n",
        "\n",
        "print('pixel max : ', max_, 'pixel min : ', min_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pixel max :  16.0 pixel min :  0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O_Ko3TST-pR"
      },
      "source": [
        "x_train = ( x_train - min_ )/( max_ - min_ ) # pixel range 변화 : (0~16) -> (0~1)\n",
        "x_test =( x_test - min_ )/( max_ - min_ ) # pixel range 변화 : (0~16) -> (0~1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhQ2CFA-Uqdm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "022006fa-0a24-4e0e-dc21-a4c0288aeb5e"
      },
      "source": [
        "max_ = x_train.max()\n",
        "min_ = x_train.min()\n",
        "print('pixel max : ', max_, 'pixel min : ', min_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pixel max :  1.0 pixel min :  0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnpStAuj2Gmu"
      },
      "source": [
        "# Q3. 실행할 때마다 트레이닝셋의 이미지 하나를 랜덤하게 시각화 하는 코드를 작성하시오.\n",
        "--------------------\n",
        "* 그 이미지가 어떤 클래스인지도 같이 출력하는 코드를 작성하시오.\n",
        "----------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "q_24UYjO2aju",
        "outputId": "0f8e2a15-ecc4-4aae-b5b3-7fa4a8f0c9d8"
      },
      "source": [
        "####################\n",
        "## your code here ##\n",
        "####################\n",
        "\n",
        "id = np.random.randint(0, len(x_train))\n",
        "\n",
        "print(f\"아래 이미지는 {y_train[id]}입니다.\")\n",
        "plt.imshow(x_train[id], cmap='Greys')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "아래 이미지는 9입니다.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAALEUlEQVR4nO3dX4hc5RnH8d+vUdma2K40oegmdHMRAlKp0SWoKUITLLGK6UWRBBQaCl6kitKCaL3qRS4NFiyCxKSCiZJGBRGrFVRaIbXuxrQ1ianpkpAEbRK6wT+QhujTi51A1LV7Zvacd84+fD8Q3D/DvM9ovp6Zs7PndUQIQB5f6/cAAOpF1EAyRA0kQ9RAMkQNJHNBE3c6f/78GB4ebuKuv6Tk2fvDhw8XW0uSTp8+XWytefPmFVtraGio2Fq2i61V0qFDh3Ty5MkpH1wjUQ8PD2t0dLSJu/6Skn/xN2zYUGwtSTpw4ECxta699tpia23cuLHYWgMDA8XWKmlkZOQrv8fTbyAZogaSIWogGaIGkiFqIBmiBpIhaiAZogaSIWogmUpR215t+4Dtg7bvb3ooAL2bNmrbcyT9VtJNkq6QtM72FU0PBqA3VY7UyyUdjIjxiDgj6WlJa5odC0CvqkQ9JOnIeZ8f7Xztc2zfaXvU9uiJEyfqmg9Al2o7URYRj0XESESMLFiwoK67BdClKlEfk7TovM8Xdr4GoIWqRP2WpCW2F9u+SNJaSc83OxaAXk17kYSIOGv7LkkvS5ojaUtE7G18MgA9qXTlk4h4UdKLDc8CoAa8owxIhqiBZIgaSIaogWSIGkiGqIFkiBpIppEdOkrasmVLsbUmJiaKrSVJO3bsKLbWbbfdVmytBx98sNhaDz30ULG12oIjNZAMUQPJEDWQDFEDyRA1kAxRA8kQNZAMUQPJEDWQDFEDyVTZoWOL7eO23ykxEICZqXKk/p2k1Q3PAaAm00YdEX+S9J8CswCoQW2vqdl2B2gHtt0BkuHsN5AMUQPJVPmR1lOSdklaavuo7Z81PxaAXlXZS2tdiUEA1IOn30AyRA0kQ9RAMkQNJEPUQDJEDSRD1EAys37bnZIuvfTSouvNnTu32FpLly4tttamTZuKrbVx48Zia0nSwMBA0fWmwpEaSIaogWSIGkiGqIFkiBpIhqiBZIgaSIaogWSIGkiGqIFkqlyjbJHt12zvs73X9j0lBgPQmyrv/T4r6ZcRsdv2JZLGbL8SEfsang1AD6psu/N+ROzufPyRpP2ShpoeDEBvunpNbXtY0jJJb07xPbbdAVqgctS250l6RtK9EfHhF7/PtjtAO1SK2vaFmgx6W0Q82+xIAGaiytlvS3pc0v6IKPfb7QB6UuVIvULSHZJW2t7T+fOjhucC0KMq2+68IckFZgFQA95RBiRD1EAyRA0kQ9RAMkQNJEPUQDJEDSRD1EAyjoja73RkZCRGR0drv9+pnD59usg6knT55ZcXW0uSJiYmiq6XUel/h4ODg0XWGRkZ0ejo6JRvCuNIDSRD1EAyRA0kQ9RAMkQNJEPUQDJEDSRD1EAyRA0kU+XCgwO2/2r7b51td35dYjAAvamy7c5/Ja2MiI87lwp+w/YfIuIvDc8GoAdVLjwYkj7ufHph50/9bxgHUIuqF/OfY3uPpOOSXokItt0BWqpS1BHxaURcJWmhpOW2vzvFbdh2B2iBrs5+R8QpSa9JWt3MOABmqsrZ7wW2Bzsff13SjZLebXowAL2pcvb7MklP2J6jyf8J7IiIF5odC0Cvqpz9/rsm96QGMAvwjjIgGaIGkiFqIBmiBpIhaiAZogaSIWogGaIGkqnyjrJWGxgYKLbW+Ph4sbUkafv27cXWWrNmTbG1tm7dWmytsbGxYmtJ0qpVq4quNxWO1EAyRA0kQ9RAMkQNJEPUQDJEDSRD1EAyRA0kQ9RAMkQNJFM56s4F/d+2zUUHgRbr5kh9j6T9TQ0CoB5Vt91ZKOlmSZubHQfATFU9Uj8s6T5Jn33VDdhLC2iHKjt03CLpeET8399hYy8toB2qHKlXSLrV9iFJT0taafvJRqcC0LNpo46IByJiYUQMS1or6dWIuL3xyQD0hJ9TA8l0dTmjiHhd0uuNTAKgFhypgWSIGkiGqIFkiBpIhqiBZIgaSIaogWRm/bY7JQ0ODhZdb8OGDUXXK2XJkiXF1tq1a1extSS23QHQAKIGkiFqIBmiBpIhaiAZogaSIWogGaIGkiFqIBmiBpKp9DbRzpVEP5L0qaSzETHS5FAAetfNe79/EBEnG5sEQC14+g0kUzXqkPRH22O275zqBmy7A7RD1ai/HxFXS7pJ0s9t3/DFG7DtDtAOlaKOiGOdfx6X9Jyk5U0OBaB3VTbIm2v7knMfS/qhpHeaHgxAb6qc/f62pOdsn7v99oh4qdGpAPRs2qgjYlzS9wrMAqAG/EgLSIaogWSIGkiGqIFkiBpIhqiBZIgaSGbWb7tz6tSpYmutX7++2FqS9MgjjxRba2hoqNha7733XrG1xsbGiq3VFhypgWSIGkiGqIFkiBpIhqiBZIgaSIaogWSIGkiGqIFkiBpIplLUtgdt77T9ru39tq9rejAAvan63u/fSHopIn5i+yJJFzc4E4AZmDZq29+UdIOkn0pSRJyRdKbZsQD0qsrT78WSTkjaavtt25s71//+HLbdAdqhStQXSLpa0qMRsUzSJ5Lu/+KN2HYHaIcqUR+VdDQi3ux8vlOTkQNooWmjjogPJB2xvbTzpVWS9jU6FYCeVT37fbekbZ0z3+OSyl4CBEBllaKOiD2SRhqeBUANeEcZkAxRA8kQNZAMUQPJEDWQDFEDyRA1kAxRA8nM+r20BgYGiq11zTXXFFtLkq688spia01MTBRb6/rrry+21o4dO4qt1RYcqYFkiBpIhqiBZIgaSIaogWSIGkiGqIFkiBpIhqiBZKaN2vZS23vO+/Oh7XtLDAege9O+TTQiDki6SpJsz5F0TNJzDc8FoEfdPv1eJelfEXG4iWEAzFy3Ua+V9NRU32DbHaAdKkfdueb3rZJ+P9X32XYHaIdujtQ3SdodEf9uahgAM9dN1Ov0FU+9AbRHpag7W9feKOnZZscBMFNVt935RNK3Gp4FQA14RxmQDFEDyRA1kAxRA8kQNZAMUQPJEDWQDFEDyTgi6r9T+4Skbn89c76kk7UP0w5ZHxuPq3++ExFT/uZUI1H3wvZoRIz0e44mZH1sPK524uk3kAxRA8m0KerH+j1Ag7I+Nh5XC7XmNTWAerTpSA2gBkQNJNOKqG2vtn3A9kHb9/d7njrYXmT7Ndv7bO+1fU+/Z6qT7Tm237b9Qr9nqZPtQds7bb9re7/t6/o9U7f6/pq6s0HAPzV5uaSjkt6StC4i9vV1sBmyfZmkyyJit+1LJI1J+vFsf1zn2P6FpBFJ34iIW/o9T11sPyHpzxGxuXMF3Ysj4lS/5+pGG47UyyUdjIjxiDgj6WlJa/o804xFxPsRsbvz8UeS9ksa6u9U9bC9UNLNkjb3e5Y62f6mpBskPS5JEXFmtgUttSPqIUlHzvv8qJL85T/H9rCkZZLe7O8ktXlY0n2SPuv3IDVbLOmEpK2dlxabOxfdnFXaEHVqtudJekbSvRHxYb/nmSnbt0g6HhFj/Z6lARdIulrSoxGxTNInkmbdOZ42RH1M0qLzPl/Y+dqsZ/tCTQa9LSKyXF55haRbbR/S5Eullbaf7O9ItTkq6WhEnHtGtVOTkc8qbYj6LUlLbC/unJhYK+n5Ps80Y7atyddm+yNiU7/nqUtEPBARCyNiWJP/rV6NiNv7PFYtIuIDSUdsL+18aZWkWXdis9J1v5sUEWdt3yXpZUlzJG2JiL19HqsOKyTdIekftvd0vvariHixjzNhendL2tY5wIxLWt/nebrW9x9pAahXG55+A6gRUQPJEDWQDFEDyRA1kAxRA8kQNZDM/wAL18BNpPrZ2AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tf9hpt_21LC"
      },
      "source": [
        "# Q4. y들을 원핫인코딩 하시오.\n",
        "----------------\n",
        "* 모든 전처리 규칙은 트레이닝셋을 이용하여 찾는다.\n",
        "--------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2eO9t66VrrE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d00df8b-1edd-4a55-8636-587eec994589"
      },
      "source": [
        "tf.one_hot([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], depth=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10, 10), dtype=float32, numpy=\n",
              "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ujn3mJa3ISt"
      },
      "source": [
        "####################\n",
        "## your code here ##\n",
        "####################\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "num_ca = len(set(y_train)) #num_ca = 10\n",
        "\n",
        "y_train = to_categorical(y_train, num_ca)\n",
        "y_test = to_categorical(y_test, num_ca)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKJ6Dsv1WHmb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd7f5fa7-4cc5-4a53-e329-8fd019fa34e8"
      },
      "source": [
        "y_train[:3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNfKuylW4iuv"
      },
      "source": [
        "# Q5. 다음 조건에 맞추어 뉴럴넷을 모델링 하시오.\n",
        "------------------------------\n",
        "* model1 에 모델을 선언해둔다.\n",
        "* 컴파일까지 마친다.\n",
        "    - 모니터링용 지표로 accuracy를 둔다.\n",
        "* 모델 구조는 아래와 같다.\n",
        "    - x의 모양에 맞는 적절한 인풋레이어 \n",
        "    - Fully connected layer로 연결하기 위한 모양변환 레이어 # Flatten 쓰시오.\n",
        "    - Fully connected layer, 64개 노드, swish\n",
        "    - Fully connected layer, 64개 노드, swish\n",
        "    - 적절한 아웃풋 레이어 #분류할 클래스 개수에 맞는 Dense\n",
        "-----------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gP5zJDHK5GT2",
        "outputId": "5bd8a214-cb0e-4fd4-be84-be29ea860a35"
      },
      "source": [
        "####################\n",
        "## your code here ##\n",
        "####################\n",
        "\n",
        "keras.backend.clear_session()\n",
        "\n",
        "il = keras.layers.Input(shape=(8,8)) #input의 shape 정보. placeholder tensor\n",
        "hl = keras.layers.Flatten()(il) # (8,8) -> 64. vectorization\n",
        "hl = keras.layers.Dense(64, 'swish')(hl) # 은닉층1\n",
        "hl = keras.layers.Dense(64, 'swish')(hl) # 은닉층2\n",
        "ol = keras.layers.Dense(10, 'softmax')(hl) #출력층(확률분포 출력). 클래스 10개 = 퍼셉트론 10개\n",
        "\n",
        "model1 = keras.models.Model(il, ol) # functional API로 모델 빌드\n",
        "\n",
        "model1.compile(loss='categorical_crossentropy', #label을 one-hot coding해줘서 사용.\n",
        "              optimizer='adam', #경사하강법의 업그레이드된 알고리즘.\n",
        "              metrics=['accuracy']) #학습과정 중 모델 성능 변화\n",
        "\n",
        "model1.summary() #모델의 레이어 정보를 출력"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 8, 8)]            0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 8,970\n",
            "Trainable params: 8,970\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nBHkekK5lUF"
      },
      "source": [
        "# Q6. 다음 조건에 맞추어 model1을 얼리스토핑을 이용하여 학습시키시오.\n",
        "--------------\n",
        "* epochs = 10000 #학습데이터 전체를 몇 번 학습할 것인가?\n",
        "* batch size는 256 #1회 학습 시 사용할 데이터의 수\n",
        "* 10번 연속 개선이 없으면 stop\n",
        "    - loss가 유지만 되어도 개선됨으로 간주\n",
        "* 얼리스토핑시, 가장 성능이 좋았던 가중치로 복구.\n",
        "* 벨리데이션 셋은, 트레이닝 셋의 15%를 사용.\n",
        "----------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzT6ssUq59GI",
        "outputId": "7ea55438-985b-49de-9be1-6c35984236f2"
      },
      "source": [
        "####################\n",
        "## your code here ##\n",
        "####################\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', #validation loss를 감시해서 판단.\n",
        "                   min_delta=0,  #개선의 기준이 0(유지만 해도 개선으로 인정).\n",
        "                   patience=10, #몇 epoch 동안 개선이 없으면 학습을 정지할 것인가?\n",
        "                   verbose=1, #출력여부\n",
        "                   restore_best_weights=True) #얼리스토핑 시 가장 좋은 가중치로 복구\n",
        "\n",
        "model1.fit(x_train, y_train, \n",
        "           epochs=10000, #epoch 횟수\n",
        "           batch_size=256, #batch의 크기\n",
        "           validation_split=0.15, #학습셋의 15%를 validation 데이터로 사용. 학습셋은 85%\n",
        "           callbacks=[es], #사용할 callback 함수의 리스트 (얼리스타핑을 사용) \n",
        "           verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10000\n",
            "6/6 [==============================] - 3s 57ms/step - loss: 2.2401 - accuracy: 0.1579 - val_loss: 2.1899 - val_accuracy: 0.2757\n",
            "Epoch 2/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 2.1566 - accuracy: 0.3945 - val_loss: 2.1048 - val_accuracy: 0.5802\n",
            "Epoch 3/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 2.0691 - accuracy: 0.6063 - val_loss: 2.0089 - val_accuracy: 0.6667\n",
            "Epoch 4/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 1.9676 - accuracy: 0.6659 - val_loss: 1.8975 - val_accuracy: 0.6955\n",
            "Epoch 5/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 1.8478 - accuracy: 0.6849 - val_loss: 1.7600 - val_accuracy: 0.6914\n",
            "Epoch 6/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 1.7022 - accuracy: 0.6994 - val_loss: 1.6010 - val_accuracy: 0.7037\n",
            "Epoch 7/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 1.5326 - accuracy: 0.7256 - val_loss: 1.4204 - val_accuracy: 0.7284\n",
            "Epoch 8/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 1.3406 - accuracy: 0.7504 - val_loss: 1.2190 - val_accuracy: 0.7531\n",
            "Epoch 9/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 1.1339 - accuracy: 0.7904 - val_loss: 1.0115 - val_accuracy: 0.8230\n",
            "Epoch 10/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.9286 - accuracy: 0.8377 - val_loss: 0.8072 - val_accuracy: 0.8683\n",
            "Epoch 11/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.7418 - accuracy: 0.8763 - val_loss: 0.6298 - val_accuracy: 0.9053\n",
            "Epoch 12/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.5920 - accuracy: 0.8937 - val_loss: 0.5032 - val_accuracy: 0.9095\n",
            "Epoch 13/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.4820 - accuracy: 0.9017 - val_loss: 0.4139 - val_accuracy: 0.9177\n",
            "Epoch 14/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.4022 - accuracy: 0.9090 - val_loss: 0.3493 - val_accuracy: 0.9136\n",
            "Epoch 15/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.3482 - accuracy: 0.9192 - val_loss: 0.3038 - val_accuracy: 0.9342\n",
            "Epoch 16/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.3064 - accuracy: 0.9265 - val_loss: 0.2761 - val_accuracy: 0.9218\n",
            "Epoch 17/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.2753 - accuracy: 0.9309 - val_loss: 0.2491 - val_accuracy: 0.9300\n",
            "Epoch 18/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.2517 - accuracy: 0.9352 - val_loss: 0.2308 - val_accuracy: 0.9465\n",
            "Epoch 19/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.2315 - accuracy: 0.9374 - val_loss: 0.2191 - val_accuracy: 0.9218\n",
            "Epoch 20/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.2149 - accuracy: 0.9454 - val_loss: 0.2045 - val_accuracy: 0.9424\n",
            "Epoch 21/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1997 - accuracy: 0.9447 - val_loss: 0.1996 - val_accuracy: 0.9465\n",
            "Epoch 22/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1864 - accuracy: 0.9505 - val_loss: 0.1873 - val_accuracy: 0.9342\n",
            "Epoch 23/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1748 - accuracy: 0.9563 - val_loss: 0.1844 - val_accuracy: 0.9383\n",
            "Epoch 24/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1645 - accuracy: 0.9607 - val_loss: 0.1747 - val_accuracy: 0.9506\n",
            "Epoch 25/10000\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1552 - accuracy: 0.9629 - val_loss: 0.1738 - val_accuracy: 0.9383\n",
            "Epoch 26/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1482 - accuracy: 0.9709 - val_loss: 0.1668 - val_accuracy: 0.9424\n",
            "Epoch 27/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1418 - accuracy: 0.9694 - val_loss: 0.1616 - val_accuracy: 0.9424\n",
            "Epoch 28/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1341 - accuracy: 0.9680 - val_loss: 0.1609 - val_accuracy: 0.9424\n",
            "Epoch 29/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1299 - accuracy: 0.9702 - val_loss: 0.1563 - val_accuracy: 0.9424\n",
            "Epoch 30/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1236 - accuracy: 0.9745 - val_loss: 0.1540 - val_accuracy: 0.9465\n",
            "Epoch 31/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1177 - accuracy: 0.9789 - val_loss: 0.1530 - val_accuracy: 0.9465\n",
            "Epoch 32/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1129 - accuracy: 0.9782 - val_loss: 0.1476 - val_accuracy: 0.9506\n",
            "Epoch 33/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1090 - accuracy: 0.9796 - val_loss: 0.1465 - val_accuracy: 0.9465\n",
            "Epoch 34/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1043 - accuracy: 0.9789 - val_loss: 0.1456 - val_accuracy: 0.9465\n",
            "Epoch 35/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1008 - accuracy: 0.9803 - val_loss: 0.1430 - val_accuracy: 0.9465\n",
            "Epoch 36/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0968 - accuracy: 0.9825 - val_loss: 0.1393 - val_accuracy: 0.9547\n",
            "Epoch 37/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0977 - accuracy: 0.9803 - val_loss: 0.1466 - val_accuracy: 0.9424\n",
            "Epoch 38/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0918 - accuracy: 0.9833 - val_loss: 0.1369 - val_accuracy: 0.9506\n",
            "Epoch 39/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0905 - accuracy: 0.9818 - val_loss: 0.1397 - val_accuracy: 0.9506\n",
            "Epoch 40/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0852 - accuracy: 0.9862 - val_loss: 0.1404 - val_accuracy: 0.9506\n",
            "Epoch 41/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0827 - accuracy: 0.9862 - val_loss: 0.1350 - val_accuracy: 0.9506\n",
            "Epoch 42/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0803 - accuracy: 0.9869 - val_loss: 0.1342 - val_accuracy: 0.9547\n",
            "Epoch 43/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0767 - accuracy: 0.9884 - val_loss: 0.1364 - val_accuracy: 0.9465\n",
            "Epoch 44/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0769 - accuracy: 0.9891 - val_loss: 0.1377 - val_accuracy: 0.9506\n",
            "Epoch 45/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0750 - accuracy: 0.9869 - val_loss: 0.1312 - val_accuracy: 0.9506\n",
            "Epoch 46/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0710 - accuracy: 0.9891 - val_loss: 0.1328 - val_accuracy: 0.9547\n",
            "Epoch 47/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0702 - accuracy: 0.9898 - val_loss: 0.1331 - val_accuracy: 0.9506\n",
            "Epoch 48/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0672 - accuracy: 0.9898 - val_loss: 0.1281 - val_accuracy: 0.9547\n",
            "Epoch 49/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0654 - accuracy: 0.9905 - val_loss: 0.1285 - val_accuracy: 0.9547\n",
            "Epoch 50/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0636 - accuracy: 0.9905 - val_loss: 0.1296 - val_accuracy: 0.9547\n",
            "Epoch 51/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0639 - accuracy: 0.9905 - val_loss: 0.1262 - val_accuracy: 0.9547\n",
            "Epoch 52/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0615 - accuracy: 0.9898 - val_loss: 0.1254 - val_accuracy: 0.9588\n",
            "Epoch 53/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0600 - accuracy: 0.9891 - val_loss: 0.1317 - val_accuracy: 0.9588\n",
            "Epoch 54/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0596 - accuracy: 0.9905 - val_loss: 0.1250 - val_accuracy: 0.9588\n",
            "Epoch 55/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0567 - accuracy: 0.9905 - val_loss: 0.1229 - val_accuracy: 0.9588\n",
            "Epoch 56/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0558 - accuracy: 0.9905 - val_loss: 0.1259 - val_accuracy: 0.9588\n",
            "Epoch 57/10000\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0537 - accuracy: 0.9905 - val_loss: 0.1210 - val_accuracy: 0.9588\n",
            "Epoch 58/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0529 - accuracy: 0.9905 - val_loss: 0.1251 - val_accuracy: 0.9630\n",
            "Epoch 59/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0528 - accuracy: 0.9905 - val_loss: 0.1228 - val_accuracy: 0.9630\n",
            "Epoch 60/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0504 - accuracy: 0.9905 - val_loss: 0.1191 - val_accuracy: 0.9630\n",
            "Epoch 61/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0506 - accuracy: 0.9898 - val_loss: 0.1242 - val_accuracy: 0.9630\n",
            "Epoch 62/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0489 - accuracy: 0.9920 - val_loss: 0.1209 - val_accuracy: 0.9671\n",
            "Epoch 63/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0478 - accuracy: 0.9920 - val_loss: 0.1285 - val_accuracy: 0.9671\n",
            "Epoch 64/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0469 - accuracy: 0.9905 - val_loss: 0.1190 - val_accuracy: 0.9671\n",
            "Epoch 65/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0476 - accuracy: 0.9898 - val_loss: 0.1211 - val_accuracy: 0.9630\n",
            "Epoch 66/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0440 - accuracy: 0.9934 - val_loss: 0.1171 - val_accuracy: 0.9630\n",
            "Epoch 67/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0439 - accuracy: 0.9927 - val_loss: 0.1278 - val_accuracy: 0.9671\n",
            "Epoch 68/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0427 - accuracy: 0.9927 - val_loss: 0.1209 - val_accuracy: 0.9671\n",
            "Epoch 69/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0419 - accuracy: 0.9920 - val_loss: 0.1168 - val_accuracy: 0.9630\n",
            "Epoch 70/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0402 - accuracy: 0.9934 - val_loss: 0.1279 - val_accuracy: 0.9671\n",
            "Epoch 71/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0421 - accuracy: 0.9920 - val_loss: 0.1209 - val_accuracy: 0.9630\n",
            "Epoch 72/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0397 - accuracy: 0.9949 - val_loss: 0.1146 - val_accuracy: 0.9671\n",
            "Epoch 73/10000\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.0392 - accuracy: 0.9942 - val_loss: 0.1218 - val_accuracy: 0.9671\n",
            "Epoch 74/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0370 - accuracy: 0.9956 - val_loss: 0.1131 - val_accuracy: 0.9671\n",
            "Epoch 75/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0373 - accuracy: 0.9956 - val_loss: 0.1254 - val_accuracy: 0.9671\n",
            "Epoch 76/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0365 - accuracy: 0.9934 - val_loss: 0.1201 - val_accuracy: 0.9630\n",
            "Epoch 77/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0374 - accuracy: 0.9942 - val_loss: 0.1164 - val_accuracy: 0.9630\n",
            "Epoch 78/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0356 - accuracy: 0.9949 - val_loss: 0.1293 - val_accuracy: 0.9588\n",
            "Epoch 79/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0351 - accuracy: 0.9949 - val_loss: 0.1171 - val_accuracy: 0.9630\n",
            "Epoch 80/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0339 - accuracy: 0.9956 - val_loss: 0.1176 - val_accuracy: 0.9671\n",
            "Epoch 81/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0325 - accuracy: 0.9949 - val_loss: 0.1231 - val_accuracy: 0.9671\n",
            "Epoch 82/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0314 - accuracy: 0.9956 - val_loss: 0.1165 - val_accuracy: 0.9671\n",
            "Epoch 83/10000\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.0311 - accuracy: 0.9964 - val_loss: 0.1191 - val_accuracy: 0.9671\n",
            "Epoch 84/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0307 - accuracy: 0.9956 - val_loss: 0.1194 - val_accuracy: 0.9671\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00084: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4e9d20c850>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34VQNpKS6lsg"
      },
      "source": [
        "# Q7. 다음 조건에 맞추어 뉴럴넷을 모델링 하시오.\n",
        "------------------------------\n",
        "* model2 에 모델을 선언해둔다.\n",
        "* 컴파일까지 마친다.\n",
        "    - 모니터링용 지표로 accuracy를 둔다.\n",
        "* 모델 구조는 아래와 같다.\n",
        "    - x의 모양에 맞는 적절한 인풋레이어\n",
        "    - Fully connected layer로 연결하기 위한 모양변환 레이어\n",
        "    - Fully connected layer, 128개 노드, swish\n",
        "    - Batch normalization\n",
        "    - drop out : drop rate 25%\n",
        "    - Fully connected layer, 128개 노드, swish\n",
        "    - Batch normalization\n",
        "    - drop out : drop rate 25%\n",
        "    - 적절한 아웃풋 레이어\n",
        "-----------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anm52rbJ6lsg",
        "outputId": "4d06083d-2854-4eb0-e2a9-7993d4fe75b8"
      },
      "source": [
        "####################\n",
        "## your code here ##\n",
        "####################\n",
        "\n",
        "il = keras.layers.Input(shape=(8,8))\n",
        "hl = keras.layers.Flatten()(il)\n",
        "hl = keras.layers.Dense(128, 'swish')(hl) #더 많은 퍼셉트론 사용 -> 성능개선 but 과적합 우려\n",
        "hl = keras.layers.BatchNormalization()(hl) #과적합을 줄여주는 레이어\n",
        "hl = keras.layers.Dropout(0.25)(hl) #과적합을 줄여주는 레이어\n",
        "hl = keras.layers.Dense(128, 'swish')(hl) #더 많은 퍼셉트론 사용 -> 성능개선 but 과적합 우려\n",
        "hl = keras.layers.BatchNormalization()(hl) #과적합을 줄여주는 레이어\n",
        "hl = keras.layers.Dropout(0.25)(hl) #과적합을 줄여주는 레이어\n",
        "ol = keras.layers.Dense(10, 'softmax')(hl)\n",
        "\n",
        "model2 = keras.models.Model(il, ol)\n",
        "\n",
        "model2.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model2.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 8, 8)]            0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 128)               8320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 27,146\n",
            "Trainable params: 26,634\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXoVesOe8cuR"
      },
      "source": [
        "# Q8. 다음 조건에 맞추어 model2을 얼리스토핑을 이용하여 학습시키시오.\n",
        "--------------\n",
        "* epochs = 10000\n",
        "* batch size는 256\n",
        "* 10번 연속 개선이 없으면 stop\n",
        "    - loss가 유지만 되어도 개선됨으로 간주\n",
        "* 얼리스토핑시, 가장 성능이 좋았던 가중치로 복구.\n",
        "* 벨리데이션 셋은, 트레이닝 셋의 15%를 사용.\n",
        "----------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkLRUsyV8cuT",
        "outputId": "f7be65b9-22f6-46e7-fb33-45eec37a15a7"
      },
      "source": [
        "####################\n",
        "## your code here ##\n",
        "####################\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss',\n",
        "                   min_delta=0, patience=10,\n",
        "                   verbose=1, restore_best_weights=True)\n",
        "\n",
        "model2.fit(x_train, y_train, epochs=10000, batch_size=256, validation_split=0.15,\n",
        "           callbacks=[es], verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10000\n",
            "6/6 [==============================] - 1s 49ms/step - loss: 2.4193 - accuracy: 0.2460 - val_loss: 2.1450 - val_accuracy: 0.4774\n",
            "Epoch 2/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 1.1178 - accuracy: 0.6303 - val_loss: 2.0140 - val_accuracy: 0.6708\n",
            "Epoch 3/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.6749 - accuracy: 0.7918 - val_loss: 1.9142 - val_accuracy: 0.6420\n",
            "Epoch 4/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.4538 - accuracy: 0.8595 - val_loss: 1.8374 - val_accuracy: 0.6008\n",
            "Epoch 5/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.3566 - accuracy: 0.8996 - val_loss: 1.7762 - val_accuracy: 0.5967\n",
            "Epoch 6/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.3056 - accuracy: 0.9025 - val_loss: 1.7238 - val_accuracy: 0.6173\n",
            "Epoch 7/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2551 - accuracy: 0.9250 - val_loss: 1.6770 - val_accuracy: 0.6584\n",
            "Epoch 8/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.2186 - accuracy: 0.9410 - val_loss: 1.6304 - val_accuracy: 0.7325\n",
            "Epoch 9/10000\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.2071 - accuracy: 0.9389 - val_loss: 1.5873 - val_accuracy: 0.7531\n",
            "Epoch 10/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2026 - accuracy: 0.9381 - val_loss: 1.5456 - val_accuracy: 0.7860\n",
            "Epoch 11/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1646 - accuracy: 0.9556 - val_loss: 1.5063 - val_accuracy: 0.8107\n",
            "Epoch 12/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1621 - accuracy: 0.9534 - val_loss: 1.4684 - val_accuracy: 0.8230\n",
            "Epoch 13/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1512 - accuracy: 0.9636 - val_loss: 1.4307 - val_accuracy: 0.8189\n",
            "Epoch 14/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1565 - accuracy: 0.9549 - val_loss: 1.3943 - val_accuracy: 0.8189\n",
            "Epoch 15/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1240 - accuracy: 0.9680 - val_loss: 1.3586 - val_accuracy: 0.8189\n",
            "Epoch 16/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1374 - accuracy: 0.9622 - val_loss: 1.3200 - val_accuracy: 0.8272\n",
            "Epoch 17/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1220 - accuracy: 0.9709 - val_loss: 1.2796 - val_accuracy: 0.8313\n",
            "Epoch 18/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.1187 - accuracy: 0.9687 - val_loss: 1.2363 - val_accuracy: 0.8477\n",
            "Epoch 19/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1092 - accuracy: 0.9702 - val_loss: 1.1940 - val_accuracy: 0.8601\n",
            "Epoch 20/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1145 - accuracy: 0.9694 - val_loss: 1.1557 - val_accuracy: 0.8560\n",
            "Epoch 21/10000\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1074 - accuracy: 0.9694 - val_loss: 1.1153 - val_accuracy: 0.8601\n",
            "Epoch 22/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0964 - accuracy: 0.9818 - val_loss: 1.0747 - val_accuracy: 0.8642\n",
            "Epoch 23/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0883 - accuracy: 0.9818 - val_loss: 1.0307 - val_accuracy: 0.8683\n",
            "Epoch 24/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0861 - accuracy: 0.9760 - val_loss: 0.9859 - val_accuracy: 0.8807\n",
            "Epoch 25/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0882 - accuracy: 0.9789 - val_loss: 0.9428 - val_accuracy: 0.8930\n",
            "Epoch 26/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0829 - accuracy: 0.9782 - val_loss: 0.9024 - val_accuracy: 0.8971\n",
            "Epoch 27/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0714 - accuracy: 0.9840 - val_loss: 0.8635 - val_accuracy: 0.8971\n",
            "Epoch 28/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0823 - accuracy: 0.9796 - val_loss: 0.8252 - val_accuracy: 0.9218\n",
            "Epoch 29/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0707 - accuracy: 0.9840 - val_loss: 0.7864 - val_accuracy: 0.9177\n",
            "Epoch 30/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0685 - accuracy: 0.9862 - val_loss: 0.7437 - val_accuracy: 0.9300\n",
            "Epoch 31/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0621 - accuracy: 0.9869 - val_loss: 0.7058 - val_accuracy: 0.9300\n",
            "Epoch 32/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0695 - accuracy: 0.9803 - val_loss: 0.6705 - val_accuracy: 0.9218\n",
            "Epoch 33/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0580 - accuracy: 0.9884 - val_loss: 0.6362 - val_accuracy: 0.9383\n",
            "Epoch 34/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0632 - accuracy: 0.9869 - val_loss: 0.6036 - val_accuracy: 0.9300\n",
            "Epoch 35/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0640 - accuracy: 0.9854 - val_loss: 0.5725 - val_accuracy: 0.9424\n",
            "Epoch 36/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0599 - accuracy: 0.9884 - val_loss: 0.5435 - val_accuracy: 0.9506\n",
            "Epoch 37/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0566 - accuracy: 0.9854 - val_loss: 0.5152 - val_accuracy: 0.9506\n",
            "Epoch 38/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0582 - accuracy: 0.9854 - val_loss: 0.4862 - val_accuracy: 0.9465\n",
            "Epoch 39/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0447 - accuracy: 0.9942 - val_loss: 0.4569 - val_accuracy: 0.9424\n",
            "Epoch 40/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0514 - accuracy: 0.9891 - val_loss: 0.4287 - val_accuracy: 0.9424\n",
            "Epoch 41/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0578 - accuracy: 0.9876 - val_loss: 0.4012 - val_accuracy: 0.9588\n",
            "Epoch 42/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0489 - accuracy: 0.9876 - val_loss: 0.3759 - val_accuracy: 0.9547\n",
            "Epoch 43/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0479 - accuracy: 0.9891 - val_loss: 0.3526 - val_accuracy: 0.9588\n",
            "Epoch 44/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0492 - accuracy: 0.9840 - val_loss: 0.3316 - val_accuracy: 0.9630\n",
            "Epoch 45/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0446 - accuracy: 0.9891 - val_loss: 0.3128 - val_accuracy: 0.9630\n",
            "Epoch 46/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0422 - accuracy: 0.9898 - val_loss: 0.2929 - val_accuracy: 0.9630\n",
            "Epoch 47/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0434 - accuracy: 0.9934 - val_loss: 0.2744 - val_accuracy: 0.9588\n",
            "Epoch 48/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0431 - accuracy: 0.9920 - val_loss: 0.2575 - val_accuracy: 0.9671\n",
            "Epoch 49/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.0407 - accuracy: 0.9913 - val_loss: 0.2408 - val_accuracy: 0.9671\n",
            "Epoch 50/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0396 - accuracy: 0.9942 - val_loss: 0.2228 - val_accuracy: 0.9753\n",
            "Epoch 51/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0370 - accuracy: 0.9942 - val_loss: 0.2074 - val_accuracy: 0.9712\n",
            "Epoch 52/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0375 - accuracy: 0.9949 - val_loss: 0.1956 - val_accuracy: 0.9753\n",
            "Epoch 53/10000\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.0368 - accuracy: 0.9942 - val_loss: 0.1854 - val_accuracy: 0.9753\n",
            "Epoch 54/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0412 - accuracy: 0.9905 - val_loss: 0.1759 - val_accuracy: 0.9753\n",
            "Epoch 55/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0365 - accuracy: 0.9942 - val_loss: 0.1663 - val_accuracy: 0.9794\n",
            "Epoch 56/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0321 - accuracy: 0.9964 - val_loss: 0.1572 - val_accuracy: 0.9794\n",
            "Epoch 57/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0387 - accuracy: 0.9913 - val_loss: 0.1495 - val_accuracy: 0.9794\n",
            "Epoch 58/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0378 - accuracy: 0.9949 - val_loss: 0.1426 - val_accuracy: 0.9753\n",
            "Epoch 59/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0363 - accuracy: 0.9934 - val_loss: 0.1370 - val_accuracy: 0.9753\n",
            "Epoch 60/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0327 - accuracy: 0.9934 - val_loss: 0.1330 - val_accuracy: 0.9753\n",
            "Epoch 61/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0373 - accuracy: 0.9913 - val_loss: 0.1287 - val_accuracy: 0.9794\n",
            "Epoch 62/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0303 - accuracy: 0.9934 - val_loss: 0.1224 - val_accuracy: 0.9753\n",
            "Epoch 63/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0249 - accuracy: 0.9964 - val_loss: 0.1175 - val_accuracy: 0.9753\n",
            "Epoch 64/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0281 - accuracy: 0.9949 - val_loss: 0.1118 - val_accuracy: 0.9753\n",
            "Epoch 65/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0275 - accuracy: 0.9949 - val_loss: 0.1063 - val_accuracy: 0.9753\n",
            "Epoch 66/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0290 - accuracy: 0.9949 - val_loss: 0.1017 - val_accuracy: 0.9753\n",
            "Epoch 67/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0251 - accuracy: 0.9956 - val_loss: 0.0973 - val_accuracy: 0.9794\n",
            "Epoch 68/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0280 - accuracy: 0.9942 - val_loss: 0.0938 - val_accuracy: 0.9794\n",
            "Epoch 69/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0258 - accuracy: 0.9956 - val_loss: 0.0917 - val_accuracy: 0.9794\n",
            "Epoch 70/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0256 - accuracy: 0.9949 - val_loss: 0.0901 - val_accuracy: 0.9794\n",
            "Epoch 71/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0244 - accuracy: 0.9949 - val_loss: 0.0878 - val_accuracy: 0.9753\n",
            "Epoch 72/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0239 - accuracy: 0.9985 - val_loss: 0.0856 - val_accuracy: 0.9753\n",
            "Epoch 73/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0258 - accuracy: 0.9949 - val_loss: 0.0835 - val_accuracy: 0.9753\n",
            "Epoch 74/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0276 - accuracy: 0.9934 - val_loss: 0.0804 - val_accuracy: 0.9794\n",
            "Epoch 75/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0243 - accuracy: 0.9942 - val_loss: 0.0764 - val_accuracy: 0.9794\n",
            "Epoch 76/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0259 - accuracy: 0.9956 - val_loss: 0.0731 - val_accuracy: 0.9794\n",
            "Epoch 77/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0260 - accuracy: 0.9956 - val_loss: 0.0696 - val_accuracy: 0.9753\n",
            "Epoch 78/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0225 - accuracy: 0.9964 - val_loss: 0.0666 - val_accuracy: 0.9794\n",
            "Epoch 79/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0228 - accuracy: 0.9949 - val_loss: 0.0652 - val_accuracy: 0.9794\n",
            "Epoch 80/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0231 - accuracy: 0.9956 - val_loss: 0.0640 - val_accuracy: 0.9794\n",
            "Epoch 81/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0235 - accuracy: 0.9949 - val_loss: 0.0637 - val_accuracy: 0.9794\n",
            "Epoch 82/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0189 - accuracy: 0.9985 - val_loss: 0.0633 - val_accuracy: 0.9794\n",
            "Epoch 83/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0215 - accuracy: 0.9964 - val_loss: 0.0638 - val_accuracy: 0.9753\n",
            "Epoch 84/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0189 - accuracy: 0.9985 - val_loss: 0.0642 - val_accuracy: 0.9753\n",
            "Epoch 85/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0170 - accuracy: 0.9985 - val_loss: 0.0633 - val_accuracy: 0.9753\n",
            "Epoch 86/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0228 - accuracy: 0.9942 - val_loss: 0.0626 - val_accuracy: 0.9753\n",
            "Epoch 87/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0169 - accuracy: 0.9993 - val_loss: 0.0623 - val_accuracy: 0.9753\n",
            "Epoch 88/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0226 - accuracy: 0.9942 - val_loss: 0.0621 - val_accuracy: 0.9753\n",
            "Epoch 89/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0162 - accuracy: 0.9985 - val_loss: 0.0607 - val_accuracy: 0.9753\n",
            "Epoch 90/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0161 - accuracy: 0.9985 - val_loss: 0.0597 - val_accuracy: 0.9794\n",
            "Epoch 91/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0163 - accuracy: 0.9964 - val_loss: 0.0603 - val_accuracy: 0.9794\n",
            "Epoch 92/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0205 - accuracy: 0.9956 - val_loss: 0.0626 - val_accuracy: 0.9794\n",
            "Epoch 93/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0189 - accuracy: 0.9985 - val_loss: 0.0623 - val_accuracy: 0.9794\n",
            "Epoch 94/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0180 - accuracy: 0.9985 - val_loss: 0.0627 - val_accuracy: 0.9753\n",
            "Epoch 95/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0182 - accuracy: 0.9978 - val_loss: 0.0634 - val_accuracy: 0.9753\n",
            "Epoch 96/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0217 - accuracy: 0.9934 - val_loss: 0.0643 - val_accuracy: 0.9753\n",
            "Epoch 97/10000\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.0225 - accuracy: 0.9956 - val_loss: 0.0641 - val_accuracy: 0.9753\n",
            "Epoch 98/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.0162 - accuracy: 0.9978 - val_loss: 0.0648 - val_accuracy: 0.9794\n",
            "Epoch 99/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.0177 - accuracy: 0.9978 - val_loss: 0.0665 - val_accuracy: 0.9753\n",
            "Epoch 100/10000\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.0149 - accuracy: 0.9985 - val_loss: 0.0680 - val_accuracy: 0.9753\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00100: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4e9ca45250>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFGc2QHe8kfH"
      },
      "source": [
        "# Q9. 테스트셋 위에서 두 모델을 비교하시오.\n",
        "---------------------------------\n",
        "* 두 모델의 accuracy를 출력한다.\n",
        "-------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_jhJafb8qGW",
        "outputId": "b3618435-3ebb-4389-9540-dd08e61558a6"
      },
      "source": [
        "####################\n",
        "## your code here ##\n",
        "####################\n",
        "\n",
        "model1.evaluate(x_test, y_test), model2.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 [==============================] - 0s 3ms/step - loss: 0.1284 - accuracy: 0.9667\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 0.0746 - accuracy: 0.9833\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0.12840428948402405, 0.9666666388511658],\n",
              " [0.07461224496364594, 0.9833333492279053])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH0Ru9Ey9E5k"
      },
      "source": [
        "# Q10. 성능이 더 좋은 모델을 이용해, 테스트 이미지를 시각화 하시오.\n",
        "----------------------------\n",
        "* 랜덤하게 테스트 이미지 한장을 시각화 한다.\n",
        "* 그 이미지가 실제 어떤 클래스인지 출력한다.\n",
        "    - 원핫 인코딩 된 정보를 출력하지 않는다.\n",
        "* 모델이 예측한 클래스도 같이 출력한다.\n",
        "    - 확률 정보를 출력하지 않는다.\n",
        "-----------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "_ONJOesl9VOo",
        "outputId": "811b3b7b-4c84-4767-89ff-d2a4523aad6d"
      },
      "source": [
        "####################\n",
        "## your code here ##\n",
        "####################\n",
        "\n",
        "id = id = np.random.randint(0, len(x_test))\n",
        "\n",
        "y_pred = model2.predict(x_test[id:id+1])\n",
        "\n",
        "print(f\"아래 이미지는 : {y_test[id].argmax()} 입니다.\")\n",
        "print(f\"모델의 예측은 : {y_pred.argmax()} 입니다.\")\n",
        "plt.imshow(x_test[id], cmap='Greys')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "아래 이미지는 : 8 입니다.\n",
            "모델의 예측은 : 8 입니다.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvM7G4AYcSdp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qP4lvM3RcSgP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeOhEDfYcSsx"
      },
      "source": [
        "# 또 다른 regularization 기술 : L1 regularization 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5_GBDt-cVYW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26f1f440-eadc-45f4-fd85-971f3f002286"
      },
      "source": [
        "####################\n",
        "## your code here ##\n",
        "####################\n",
        "\n",
        "il = keras.layers.Input(shape=(8,8))\n",
        "hl = keras.layers.Flatten()(il)\n",
        "hl = keras.layers.Dense(128, 'swish', \n",
        "                        kernel_regularizer=tf.keras.regularizers.L1(0.0001), \n",
        "                        bias_regularizer=tf.keras.regularizers.L1(0.0001))(hl)\n",
        "hl = keras.layers.BatchNormalization()(hl)\n",
        "hl = keras.layers.Dropout(0.25)(hl)\n",
        "hl = keras.layers.Dense(128, 'swish', \n",
        "                        kernel_regularizer=tf.keras.regularizers.L1(0.0001), \n",
        "                        bias_regularizer=tf.keras.regularizers.L1(0.0001))(hl)\n",
        "hl = keras.layers.BatchNormalization()(hl)\n",
        "hl = keras.layers.Dropout(0.25)(hl)\n",
        "ol = keras.layers.Dense(10, 'softmax')(hl)\n",
        "\n",
        "model3 = keras.models.Model(il, ol)\n",
        "\n",
        "model3.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model3.summary()\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss',\n",
        "                   min_delta=0, patience=10,\n",
        "                   verbose=1, restore_best_weights=True)\n",
        "\n",
        "model3.fit(x_train, y_train, epochs=10000, batch_size=256, validation_split=0.15,\n",
        "           callbacks=[es], verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, 8, 8)]            0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 128)               8320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 27,146\n",
            "Trainable params: 26,634\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n",
            "Epoch 1/10000\n",
            "6/6 [==============================] - 1s 52ms/step - loss: 3.0539 - accuracy: 0.1638 - val_loss: 2.3910 - val_accuracy: 0.3909\n",
            "Epoch 2/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 1.6016 - accuracy: 0.5298 - val_loss: 2.2656 - val_accuracy: 0.7449\n",
            "Epoch 3/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 1.0013 - accuracy: 0.7445 - val_loss: 2.1634 - val_accuracy: 0.8601\n",
            "Epoch 4/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.7276 - accuracy: 0.8413 - val_loss: 2.0818 - val_accuracy: 0.8807\n",
            "Epoch 5/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.6048 - accuracy: 0.8806 - val_loss: 2.0153 - val_accuracy: 0.8807\n",
            "Epoch 6/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.5390 - accuracy: 0.8974 - val_loss: 1.9582 - val_accuracy: 0.8765\n",
            "Epoch 7/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.4851 - accuracy: 0.9207 - val_loss: 1.9094 - val_accuracy: 0.8724\n",
            "Epoch 8/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.4450 - accuracy: 0.9352 - val_loss: 1.8665 - val_accuracy: 0.8724\n",
            "Epoch 9/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.4202 - accuracy: 0.9381 - val_loss: 1.8271 - val_accuracy: 0.8930\n",
            "Epoch 10/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.3899 - accuracy: 0.9469 - val_loss: 1.7885 - val_accuracy: 0.8889\n",
            "Epoch 11/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.3804 - accuracy: 0.9447 - val_loss: 1.7489 - val_accuracy: 0.8889\n",
            "Epoch 12/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.3697 - accuracy: 0.9527 - val_loss: 1.7091 - val_accuracy: 0.9053\n",
            "Epoch 13/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.3483 - accuracy: 0.9563 - val_loss: 1.6719 - val_accuracy: 0.9053\n",
            "Epoch 14/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.3403 - accuracy: 0.9658 - val_loss: 1.6340 - val_accuracy: 0.9177\n",
            "Epoch 15/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.3300 - accuracy: 0.9592 - val_loss: 1.5952 - val_accuracy: 0.9177\n",
            "Epoch 16/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.3220 - accuracy: 0.9651 - val_loss: 1.5595 - val_accuracy: 0.9177\n",
            "Epoch 17/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.3145 - accuracy: 0.9614 - val_loss: 1.5229 - val_accuracy: 0.9177\n",
            "Epoch 18/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.3148 - accuracy: 0.9680 - val_loss: 1.4880 - val_accuracy: 0.9177\n",
            "Epoch 19/10000\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.2941 - accuracy: 0.9753 - val_loss: 1.4535 - val_accuracy: 0.9177\n",
            "Epoch 20/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2973 - accuracy: 0.9760 - val_loss: 1.4177 - val_accuracy: 0.9177\n",
            "Epoch 21/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2989 - accuracy: 0.9687 - val_loss: 1.3774 - val_accuracy: 0.9177\n",
            "Epoch 22/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.2937 - accuracy: 0.9738 - val_loss: 1.3389 - val_accuracy: 0.9177\n",
            "Epoch 23/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.2882 - accuracy: 0.9731 - val_loss: 1.3006 - val_accuracy: 0.9177\n",
            "Epoch 24/10000\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.2807 - accuracy: 0.9745 - val_loss: 1.2647 - val_accuracy: 0.9177\n",
            "Epoch 25/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2698 - accuracy: 0.9789 - val_loss: 1.2258 - val_accuracy: 0.9177\n",
            "Epoch 26/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.2689 - accuracy: 0.9789 - val_loss: 1.1864 - val_accuracy: 0.9259\n",
            "Epoch 27/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.2708 - accuracy: 0.9782 - val_loss: 1.1472 - val_accuracy: 0.9300\n",
            "Epoch 28/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2631 - accuracy: 0.9811 - val_loss: 1.1060 - val_accuracy: 0.9342\n",
            "Epoch 29/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2659 - accuracy: 0.9789 - val_loss: 1.0657 - val_accuracy: 0.9342\n",
            "Epoch 30/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.2538 - accuracy: 0.9833 - val_loss: 1.0307 - val_accuracy: 0.9342\n",
            "Epoch 31/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2518 - accuracy: 0.9847 - val_loss: 0.9951 - val_accuracy: 0.9383\n",
            "Epoch 32/10000\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 0.2533 - accuracy: 0.9803 - val_loss: 0.9599 - val_accuracy: 0.9300\n",
            "Epoch 33/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2436 - accuracy: 0.9862 - val_loss: 0.9238 - val_accuracy: 0.9259\n",
            "Epoch 34/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2444 - accuracy: 0.9840 - val_loss: 0.8882 - val_accuracy: 0.9342\n",
            "Epoch 35/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2419 - accuracy: 0.9803 - val_loss: 0.8530 - val_accuracy: 0.9383\n",
            "Epoch 36/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2418 - accuracy: 0.9833 - val_loss: 0.8214 - val_accuracy: 0.9383\n",
            "Epoch 37/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2328 - accuracy: 0.9898 - val_loss: 0.7889 - val_accuracy: 0.9383\n",
            "Epoch 38/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2369 - accuracy: 0.9847 - val_loss: 0.7551 - val_accuracy: 0.9465\n",
            "Epoch 39/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.2293 - accuracy: 0.9869 - val_loss: 0.7230 - val_accuracy: 0.9506\n",
            "Epoch 40/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.2314 - accuracy: 0.9862 - val_loss: 0.6927 - val_accuracy: 0.9506\n",
            "Epoch 41/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2331 - accuracy: 0.9818 - val_loss: 0.6639 - val_accuracy: 0.9465\n",
            "Epoch 42/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2193 - accuracy: 0.9898 - val_loss: 0.6353 - val_accuracy: 0.9506\n",
            "Epoch 43/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2297 - accuracy: 0.9876 - val_loss: 0.6063 - val_accuracy: 0.9588\n",
            "Epoch 44/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2243 - accuracy: 0.9847 - val_loss: 0.5788 - val_accuracy: 0.9588\n",
            "Epoch 45/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2180 - accuracy: 0.9905 - val_loss: 0.5512 - val_accuracy: 0.9671\n",
            "Epoch 46/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2232 - accuracy: 0.9854 - val_loss: 0.5272 - val_accuracy: 0.9671\n",
            "Epoch 47/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2160 - accuracy: 0.9913 - val_loss: 0.5076 - val_accuracy: 0.9630\n",
            "Epoch 48/10000\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.2161 - accuracy: 0.9920 - val_loss: 0.4905 - val_accuracy: 0.9630\n",
            "Epoch 49/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2119 - accuracy: 0.9905 - val_loss: 0.4729 - val_accuracy: 0.9671\n",
            "Epoch 50/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2104 - accuracy: 0.9920 - val_loss: 0.4553 - val_accuracy: 0.9630\n",
            "Epoch 51/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2128 - accuracy: 0.9920 - val_loss: 0.4400 - val_accuracy: 0.9671\n",
            "Epoch 52/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2123 - accuracy: 0.9927 - val_loss: 0.4233 - val_accuracy: 0.9712\n",
            "Epoch 53/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.2079 - accuracy: 0.9905 - val_loss: 0.4076 - val_accuracy: 0.9712\n",
            "Epoch 54/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2060 - accuracy: 0.9949 - val_loss: 0.3935 - val_accuracy: 0.9712\n",
            "Epoch 55/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2074 - accuracy: 0.9913 - val_loss: 0.3797 - val_accuracy: 0.9712\n",
            "Epoch 56/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2023 - accuracy: 0.9905 - val_loss: 0.3676 - val_accuracy: 0.9671\n",
            "Epoch 57/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2052 - accuracy: 0.9949 - val_loss: 0.3562 - val_accuracy: 0.9712\n",
            "Epoch 58/10000\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.2070 - accuracy: 0.9905 - val_loss: 0.3445 - val_accuracy: 0.9712\n",
            "Epoch 59/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1996 - accuracy: 0.9956 - val_loss: 0.3324 - val_accuracy: 0.9712\n",
            "Epoch 60/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1958 - accuracy: 0.9942 - val_loss: 0.3224 - val_accuracy: 0.9712\n",
            "Epoch 61/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.2035 - accuracy: 0.9934 - val_loss: 0.3132 - val_accuracy: 0.9712\n",
            "Epoch 62/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.2007 - accuracy: 0.9927 - val_loss: 0.3063 - val_accuracy: 0.9835\n",
            "Epoch 63/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1969 - accuracy: 0.9934 - val_loss: 0.3001 - val_accuracy: 0.9835\n",
            "Epoch 64/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1968 - accuracy: 0.9927 - val_loss: 0.2948 - val_accuracy: 0.9753\n",
            "Epoch 65/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1937 - accuracy: 0.9949 - val_loss: 0.2903 - val_accuracy: 0.9671\n",
            "Epoch 66/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1936 - accuracy: 0.9942 - val_loss: 0.2835 - val_accuracy: 0.9712\n",
            "Epoch 67/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1925 - accuracy: 0.9927 - val_loss: 0.2776 - val_accuracy: 0.9753\n",
            "Epoch 68/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1882 - accuracy: 0.9949 - val_loss: 0.2714 - val_accuracy: 0.9712\n",
            "Epoch 69/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1956 - accuracy: 0.9934 - val_loss: 0.2650 - val_accuracy: 0.9753\n",
            "Epoch 70/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1889 - accuracy: 0.9934 - val_loss: 0.2600 - val_accuracy: 0.9753\n",
            "Epoch 71/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1881 - accuracy: 0.9949 - val_loss: 0.2565 - val_accuracy: 0.9794\n",
            "Epoch 72/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1856 - accuracy: 0.9956 - val_loss: 0.2536 - val_accuracy: 0.9877\n",
            "Epoch 73/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1845 - accuracy: 0.9949 - val_loss: 0.2496 - val_accuracy: 0.9835\n",
            "Epoch 74/10000\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1842 - accuracy: 0.9964 - val_loss: 0.2454 - val_accuracy: 0.9835\n",
            "Epoch 75/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1843 - accuracy: 0.9949 - val_loss: 0.2420 - val_accuracy: 0.9794\n",
            "Epoch 76/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1929 - accuracy: 0.9927 - val_loss: 0.2379 - val_accuracy: 0.9835\n",
            "Epoch 77/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1829 - accuracy: 0.9956 - val_loss: 0.2346 - val_accuracy: 0.9835\n",
            "Epoch 78/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1810 - accuracy: 0.9971 - val_loss: 0.2320 - val_accuracy: 0.9835\n",
            "Epoch 79/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1818 - accuracy: 0.9934 - val_loss: 0.2292 - val_accuracy: 0.9877\n",
            "Epoch 80/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1841 - accuracy: 0.9942 - val_loss: 0.2265 - val_accuracy: 0.9877\n",
            "Epoch 81/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1756 - accuracy: 0.9993 - val_loss: 0.2245 - val_accuracy: 0.9877\n",
            "Epoch 82/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1752 - accuracy: 0.9985 - val_loss: 0.2239 - val_accuracy: 0.9877\n",
            "Epoch 83/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1757 - accuracy: 0.9964 - val_loss: 0.2221 - val_accuracy: 0.9877\n",
            "Epoch 84/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1794 - accuracy: 0.9956 - val_loss: 0.2202 - val_accuracy: 0.9877\n",
            "Epoch 85/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1758 - accuracy: 0.9942 - val_loss: 0.2183 - val_accuracy: 0.9877\n",
            "Epoch 86/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1725 - accuracy: 0.9978 - val_loss: 0.2147 - val_accuracy: 0.9877\n",
            "Epoch 87/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1767 - accuracy: 0.9942 - val_loss: 0.2119 - val_accuracy: 0.9877\n",
            "Epoch 88/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1795 - accuracy: 0.9942 - val_loss: 0.2102 - val_accuracy: 0.9877\n",
            "Epoch 89/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1737 - accuracy: 0.9934 - val_loss: 0.2109 - val_accuracy: 0.9877\n",
            "Epoch 90/10000\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1698 - accuracy: 0.9978 - val_loss: 0.2110 - val_accuracy: 0.9877\n",
            "Epoch 91/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1723 - accuracy: 0.9956 - val_loss: 0.2109 - val_accuracy: 0.9877\n",
            "Epoch 92/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1698 - accuracy: 0.9978 - val_loss: 0.2108 - val_accuracy: 0.9877\n",
            "Epoch 93/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1650 - accuracy: 0.9993 - val_loss: 0.2101 - val_accuracy: 0.9877\n",
            "Epoch 94/10000\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1654 - accuracy: 0.9978 - val_loss: 0.2091 - val_accuracy: 0.9877\n",
            "Epoch 95/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1673 - accuracy: 0.9978 - val_loss: 0.2085 - val_accuracy: 0.9877\n",
            "Epoch 96/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1671 - accuracy: 0.9985 - val_loss: 0.2078 - val_accuracy: 0.9877\n",
            "Epoch 97/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1666 - accuracy: 0.9971 - val_loss: 0.2077 - val_accuracy: 0.9877\n",
            "Epoch 98/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1662 - accuracy: 0.9964 - val_loss: 0.2078 - val_accuracy: 0.9877\n",
            "Epoch 99/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1653 - accuracy: 0.9956 - val_loss: 0.2071 - val_accuracy: 0.9877\n",
            "Epoch 100/10000\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1608 - accuracy: 0.9971 - val_loss: 0.2078 - val_accuracy: 0.9835\n",
            "Epoch 101/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1616 - accuracy: 0.9985 - val_loss: 0.2069 - val_accuracy: 0.9835\n",
            "Epoch 102/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1662 - accuracy: 0.9964 - val_loss: 0.2058 - val_accuracy: 0.9835\n",
            "Epoch 103/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1634 - accuracy: 0.9964 - val_loss: 0.2030 - val_accuracy: 0.9835\n",
            "Epoch 104/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1661 - accuracy: 0.9949 - val_loss: 0.2023 - val_accuracy: 0.9835\n",
            "Epoch 105/10000\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 0.1649 - accuracy: 0.9971 - val_loss: 0.2047 - val_accuracy: 0.9835\n",
            "Epoch 106/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1614 - accuracy: 0.9964 - val_loss: 0.2055 - val_accuracy: 0.9835\n",
            "Epoch 107/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1624 - accuracy: 0.9949 - val_loss: 0.2054 - val_accuracy: 0.9753\n",
            "Epoch 108/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1589 - accuracy: 0.9971 - val_loss: 0.2076 - val_accuracy: 0.9753\n",
            "Epoch 109/10000\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1599 - accuracy: 0.9978 - val_loss: 0.2062 - val_accuracy: 0.9753\n",
            "Epoch 110/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1635 - accuracy: 0.9942 - val_loss: 0.2018 - val_accuracy: 0.9753\n",
            "Epoch 111/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1565 - accuracy: 0.9971 - val_loss: 0.1993 - val_accuracy: 0.9753\n",
            "Epoch 112/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1546 - accuracy: 0.9978 - val_loss: 0.1967 - val_accuracy: 0.9794\n",
            "Epoch 113/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1530 - accuracy: 0.9985 - val_loss: 0.1959 - val_accuracy: 0.9835\n",
            "Epoch 114/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1558 - accuracy: 0.9971 - val_loss: 0.1956 - val_accuracy: 0.9835\n",
            "Epoch 115/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1534 - accuracy: 0.9993 - val_loss: 0.1950 - val_accuracy: 0.9794\n",
            "Epoch 116/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1554 - accuracy: 0.9978 - val_loss: 0.1932 - val_accuracy: 0.9794\n",
            "Epoch 117/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1559 - accuracy: 0.9949 - val_loss: 0.1938 - val_accuracy: 0.9794\n",
            "Epoch 118/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1589 - accuracy: 0.9949 - val_loss: 0.1939 - val_accuracy: 0.9794\n",
            "Epoch 119/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1522 - accuracy: 0.9978 - val_loss: 0.1951 - val_accuracy: 0.9794\n",
            "Epoch 120/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1489 - accuracy: 0.9985 - val_loss: 0.1964 - val_accuracy: 0.9794\n",
            "Epoch 121/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1527 - accuracy: 0.9956 - val_loss: 0.1936 - val_accuracy: 0.9794\n",
            "Epoch 122/10000\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1495 - accuracy: 0.9985 - val_loss: 0.1927 - val_accuracy: 0.9794\n",
            "Epoch 123/10000\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1503 - accuracy: 0.9978 - val_loss: 0.1946 - val_accuracy: 0.9753\n",
            "Epoch 124/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1499 - accuracy: 0.9964 - val_loss: 0.1932 - val_accuracy: 0.9753\n",
            "Epoch 125/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1520 - accuracy: 0.9956 - val_loss: 0.1910 - val_accuracy: 0.9753\n",
            "Epoch 126/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1450 - accuracy: 0.9978 - val_loss: 0.1899 - val_accuracy: 0.9794\n",
            "Epoch 127/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1462 - accuracy: 0.9971 - val_loss: 0.1900 - val_accuracy: 0.9794\n",
            "Epoch 128/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1470 - accuracy: 0.9985 - val_loss: 0.1924 - val_accuracy: 0.9794\n",
            "Epoch 129/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1453 - accuracy: 0.9978 - val_loss: 0.1965 - val_accuracy: 0.9753\n",
            "Epoch 130/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1428 - accuracy: 1.0000 - val_loss: 0.1979 - val_accuracy: 0.9712\n",
            "Epoch 131/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1426 - accuracy: 0.9985 - val_loss: 0.1949 - val_accuracy: 0.9753\n",
            "Epoch 132/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1408 - accuracy: 0.9985 - val_loss: 0.1907 - val_accuracy: 0.9753\n",
            "Epoch 133/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1457 - accuracy: 0.9956 - val_loss: 0.1858 - val_accuracy: 0.9753\n",
            "Epoch 134/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1410 - accuracy: 0.9993 - val_loss: 0.1835 - val_accuracy: 0.9753\n",
            "Epoch 135/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1422 - accuracy: 0.9971 - val_loss: 0.1816 - val_accuracy: 0.9794\n",
            "Epoch 136/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1393 - accuracy: 0.9993 - val_loss: 0.1815 - val_accuracy: 0.9794\n",
            "Epoch 137/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1436 - accuracy: 0.9956 - val_loss: 0.1851 - val_accuracy: 0.9753\n",
            "Epoch 138/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1407 - accuracy: 0.9971 - val_loss: 0.1874 - val_accuracy: 0.9794\n",
            "Epoch 139/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1357 - accuracy: 1.0000 - val_loss: 0.1863 - val_accuracy: 0.9794\n",
            "Epoch 140/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1380 - accuracy: 0.9985 - val_loss: 0.1835 - val_accuracy: 0.9835\n",
            "Epoch 141/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1383 - accuracy: 0.9993 - val_loss: 0.1811 - val_accuracy: 0.9835\n",
            "Epoch 142/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1350 - accuracy: 0.9978 - val_loss: 0.1799 - val_accuracy: 0.9835\n",
            "Epoch 143/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1358 - accuracy: 0.9985 - val_loss: 0.1789 - val_accuracy: 0.9835\n",
            "Epoch 144/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1337 - accuracy: 0.9993 - val_loss: 0.1789 - val_accuracy: 0.9835\n",
            "Epoch 145/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1382 - accuracy: 0.9971 - val_loss: 0.1807 - val_accuracy: 0.9835\n",
            "Epoch 146/10000\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1382 - accuracy: 0.9978 - val_loss: 0.1826 - val_accuracy: 0.9835\n",
            "Epoch 147/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1353 - accuracy: 0.9964 - val_loss: 0.1829 - val_accuracy: 0.9835\n",
            "Epoch 148/10000\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1330 - accuracy: 0.9993 - val_loss: 0.1832 - val_accuracy: 0.9835\n",
            "Epoch 149/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1321 - accuracy: 0.9985 - val_loss: 0.1850 - val_accuracy: 0.9835\n",
            "Epoch 150/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1310 - accuracy: 0.9985 - val_loss: 0.1851 - val_accuracy: 0.9835\n",
            "Epoch 151/10000\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 0.1309 - accuracy: 0.9993 - val_loss: 0.1825 - val_accuracy: 0.9794\n",
            "Epoch 152/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1335 - accuracy: 0.9971 - val_loss: 0.1794 - val_accuracy: 0.9794\n",
            "Epoch 153/10000\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1313 - accuracy: 0.9978 - val_loss: 0.1764 - val_accuracy: 0.9794\n",
            "Epoch 154/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1301 - accuracy: 0.9985 - val_loss: 0.1786 - val_accuracy: 0.9794\n",
            "Epoch 155/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1314 - accuracy: 0.9971 - val_loss: 0.1818 - val_accuracy: 0.9794\n",
            "Epoch 156/10000\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1293 - accuracy: 0.9993 - val_loss: 0.1829 - val_accuracy: 0.9794\n",
            "Epoch 157/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1284 - accuracy: 0.9985 - val_loss: 0.1836 - val_accuracy: 0.9753\n",
            "Epoch 158/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1251 - accuracy: 1.0000 - val_loss: 0.1819 - val_accuracy: 0.9753\n",
            "Epoch 159/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1284 - accuracy: 0.9978 - val_loss: 0.1780 - val_accuracy: 0.9753\n",
            "Epoch 160/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1274 - accuracy: 0.9985 - val_loss: 0.1748 - val_accuracy: 0.9753\n",
            "Epoch 161/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1256 - accuracy: 0.9985 - val_loss: 0.1727 - val_accuracy: 0.9794\n",
            "Epoch 162/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1240 - accuracy: 0.9993 - val_loss: 0.1722 - val_accuracy: 0.9794\n",
            "Epoch 163/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1234 - accuracy: 0.9993 - val_loss: 0.1699 - val_accuracy: 0.9794\n",
            "Epoch 164/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1249 - accuracy: 0.9985 - val_loss: 0.1703 - val_accuracy: 0.9753\n",
            "Epoch 165/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1222 - accuracy: 0.9993 - val_loss: 0.1694 - val_accuracy: 0.9753\n",
            "Epoch 166/10000\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 0.1212 - accuracy: 0.9985 - val_loss: 0.1677 - val_accuracy: 0.9753\n",
            "Epoch 167/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1238 - accuracy: 0.9978 - val_loss: 0.1669 - val_accuracy: 0.9753\n",
            "Epoch 168/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1194 - accuracy: 0.9993 - val_loss: 0.1663 - val_accuracy: 0.9835\n",
            "Epoch 169/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1238 - accuracy: 0.9956 - val_loss: 0.1670 - val_accuracy: 0.9794\n",
            "Epoch 170/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1228 - accuracy: 0.9978 - val_loss: 0.1686 - val_accuracy: 0.9835\n",
            "Epoch 171/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1187 - accuracy: 0.9993 - val_loss: 0.1677 - val_accuracy: 0.9794\n",
            "Epoch 172/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1221 - accuracy: 0.9978 - val_loss: 0.1652 - val_accuracy: 0.9794\n",
            "Epoch 173/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1207 - accuracy: 0.9985 - val_loss: 0.1627 - val_accuracy: 0.9794\n",
            "Epoch 174/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1207 - accuracy: 0.9978 - val_loss: 0.1626 - val_accuracy: 0.9794\n",
            "Epoch 175/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1167 - accuracy: 0.9993 - val_loss: 0.1643 - val_accuracy: 0.9794\n",
            "Epoch 176/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1170 - accuracy: 0.9993 - val_loss: 0.1684 - val_accuracy: 0.9753\n",
            "Epoch 177/10000\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1170 - accuracy: 0.9993 - val_loss: 0.1677 - val_accuracy: 0.9753\n",
            "Epoch 178/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1148 - accuracy: 0.9985 - val_loss: 0.1652 - val_accuracy: 0.9753\n",
            "Epoch 179/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1157 - accuracy: 0.9985 - val_loss: 0.1660 - val_accuracy: 0.9794\n",
            "Epoch 180/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1147 - accuracy: 0.9993 - val_loss: 0.1649 - val_accuracy: 0.9753\n",
            "Epoch 181/10000\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 0.1147 - accuracy: 0.9993 - val_loss: 0.1640 - val_accuracy: 0.9794\n",
            "Epoch 182/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1141 - accuracy: 0.9985 - val_loss: 0.1634 - val_accuracy: 0.9835\n",
            "Epoch 183/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1118 - accuracy: 0.9993 - val_loss: 0.1627 - val_accuracy: 0.9794\n",
            "Epoch 184/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1145 - accuracy: 0.9964 - val_loss: 0.1607 - val_accuracy: 0.9794\n",
            "Epoch 185/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1145 - accuracy: 0.9971 - val_loss: 0.1641 - val_accuracy: 0.9753\n",
            "Epoch 186/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1128 - accuracy: 0.9964 - val_loss: 0.1839 - val_accuracy: 0.9671\n",
            "Epoch 187/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1176 - accuracy: 0.9971 - val_loss: 0.1922 - val_accuracy: 0.9630\n",
            "Epoch 188/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1115 - accuracy: 0.9993 - val_loss: 0.1796 - val_accuracy: 0.9712\n",
            "Epoch 189/10000\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1089 - accuracy: 0.9993 - val_loss: 0.1699 - val_accuracy: 0.9712\n",
            "Epoch 190/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1124 - accuracy: 0.9985 - val_loss: 0.1662 - val_accuracy: 0.9712\n",
            "Epoch 191/10000\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 0.1094 - accuracy: 0.9993 - val_loss: 0.1671 - val_accuracy: 0.9712\n",
            "Epoch 192/10000\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 0.1123 - accuracy: 0.9971 - val_loss: 0.1663 - val_accuracy: 0.9753\n",
            "Epoch 193/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1113 - accuracy: 0.9978 - val_loss: 0.1650 - val_accuracy: 0.9753\n",
            "Epoch 194/10000\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 0.1126 - accuracy: 0.9964 - val_loss: 0.1658 - val_accuracy: 0.9753\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00194: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4e9a5a2950>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93TDJPaPdAM9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf8f31f2-a078-4fc8-85e7-e333dd4f53b8"
      },
      "source": [
        "model3.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 [==============================] - 0s 3ms/step - loss: 0.1887 - accuracy: 0.9778\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.18867257237434387, 0.9777777791023254]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    }
  ]
}